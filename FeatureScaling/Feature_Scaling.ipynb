{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files:\n",
    "train = pd.read_csv(\"train_kOBLwZA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier                 0\n",
       "Item_Weight                  1463\n",
       "Item_Fat_Content                0\n",
       "Item_Visibility                 0\n",
       "Item_Type                       0\n",
       "Item_MRP                        0\n",
       "Outlet_Identifier               0\n",
       "Outlet_Establishment_Year       0\n",
       "Outlet_Size                  2410\n",
       "Outlet_Location_Type            0\n",
       "Outlet_Type                     0\n",
       "Item_Outlet_Sales               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier              0\n",
       "Item_Weight                  0\n",
       "Item_Fat_Content             0\n",
       "Item_Visibility              0\n",
       "Item_Type                    0\n",
       "Item_MRP                     0\n",
       "Outlet_Identifier            0\n",
       "Outlet_Establishment_Year    0\n",
       "Outlet_Size                  0\n",
       "Outlet_Location_Type         0\n",
       "Outlet_Type                  0\n",
       "Item_Outlet_Sales            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7060.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.857645</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>140.992782</td>\n",
       "      <td>1997.831867</td>\n",
       "      <td>2181.288914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.643456</td>\n",
       "      <td>0.051598</td>\n",
       "      <td>62.275067</td>\n",
       "      <td>8.371760</td>\n",
       "      <td>1706.499616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.555000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.290000</td>\n",
       "      <td>1985.000000</td>\n",
       "      <td>33.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.773750</td>\n",
       "      <td>0.026989</td>\n",
       "      <td>93.826500</td>\n",
       "      <td>1987.000000</td>\n",
       "      <td>834.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.053931</td>\n",
       "      <td>143.012800</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1794.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.850000</td>\n",
       "      <td>0.094585</td>\n",
       "      <td>185.643700</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>3101.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.350000</td>\n",
       "      <td>0.328391</td>\n",
       "      <td>266.888400</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>13086.964800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Item_Weight  Item_Visibility     Item_MRP  Outlet_Establishment_Year  \\\n",
       "count  7060.000000      8523.000000  8523.000000                8523.000000   \n",
       "mean     12.857645         0.066132   140.992782                1997.831867   \n",
       "std       4.643456         0.051598    62.275067                   8.371760   \n",
       "min       4.555000         0.000000    31.290000                1985.000000   \n",
       "25%       8.773750         0.026989    93.826500                1987.000000   \n",
       "50%      12.600000         0.053931   143.012800                1999.000000   \n",
       "75%      16.850000         0.094585   185.643700                2004.000000   \n",
       "max      21.350000         0.328391   266.888400                2009.000000   \n",
       "\n",
       "       Item_Outlet_Sales  \n",
       "count        8523.000000  \n",
       "mean         2181.288914  \n",
       "std          1706.499616  \n",
       "min            33.290000  \n",
       "25%           834.247400  \n",
       "50%          1794.331000  \n",
       "75%          3101.296400  \n",
       "max         13086.964800  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier              1559\n",
       "Item_Weight                   416\n",
       "Item_Fat_Content                5\n",
       "Item_Visibility              7880\n",
       "Item_Type                      16\n",
       "Item_MRP                     5938\n",
       "Outlet_Identifier              10\n",
       "Outlet_Establishment_Year       9\n",
       "Outlet_Size                     4\n",
       "Outlet_Location_Type            3\n",
       "Outlet_Type                     4\n",
       "Item_Outlet_Sales            3493\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8523, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter categorical variables\n",
    "categorical_columns = [x for x in data.dtypes.index if data.dtypes[x]=='object']\n",
    "#Exclude ID cols and source:\n",
    "categorical_columns = [x for x in categorical_columns if x not in ['Item_Identifier','Outlet_Identifier','source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequency of Categories for varible Item_Fat_Content\n",
      "Low Fat    5089\n",
      "Regular    2889\n",
      "LF          316\n",
      "reg         117\n",
      "low fat     112\n",
      "Name: Item_Fat_Content, dtype: int64\n",
      "\n",
      "Frequency of Categories for varible Item_Type\n",
      "Fruits and Vegetables    1232\n",
      "Snack Foods              1200\n",
      "Household                 910\n",
      "Frozen Foods              856\n",
      "Dairy                     682\n",
      "Canned                    649\n",
      "Baking Goods              648\n",
      "Health and Hygiene        520\n",
      "Soft Drinks               445\n",
      "Meat                      425\n",
      "Breads                    251\n",
      "Hard Drinks               214\n",
      "Others                    169\n",
      "Starchy Foods             148\n",
      "Breakfast                 110\n",
      "Seafood                    64\n",
      "Name: Item_Type, dtype: int64\n",
      "\n",
      "Frequency of Categories for varible Outlet_Size\n",
      "Medium    2793\n",
      "Small     2388\n",
      "High       932\n",
      "Name: Outlet_Size, dtype: int64\n",
      "\n",
      "Frequency of Categories for varible Outlet_Location_Type\n",
      "Tier 3    3350\n",
      "Tier 2    2785\n",
      "Tier 1    2388\n",
      "Name: Outlet_Location_Type, dtype: int64\n",
      "\n",
      "Frequency of Categories for varible Outlet_Type\n",
      "Supermarket Type1    5577\n",
      "Grocery Store        1083\n",
      "Supermarket Type3     935\n",
      "Supermarket Type2     928\n",
      "Name: Outlet_Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Print frequency of categories\n",
    "for col in categorical_columns:\n",
    "    print('\\nFrequency of Categories for varible %s'%col)\n",
    "    print(data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility',\n",
      "       'Item_Type', 'Item_MRP', 'Outlet_Identifier',\n",
      "       'Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type',\n",
      "       'Outlet_Type', 'Item_Outlet_Sales'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "X = data[['Item_Weight', 'Item_MRP']]\n",
    "y = data['Item_Outlet_Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalization with sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Train Data: \n",
      "\n",
      "\n",
      "[[0.24412027 0.37504758]\n",
      " [0.43435546 0.09165993]\n",
      " [0.66656743 0.02192283]\n",
      " ...\n",
      " [0.1947008  0.54095015]\n",
      " [0.99404585 0.85616555]\n",
      " [0.108961   0.33292325]]\n",
      "\n",
      "\n",
      "Scaled Test Data: \n",
      "\n",
      "\n",
      "[[0.96129801 0.02574614]\n",
      " [0.1947008  0.94129697]\n",
      " [0.64275082 0.6782561 ]\n",
      " ...\n",
      " [0.14855612 0.32286201]\n",
      " [0.93450432 0.6419228 ]\n",
      " [0.43435546 0.2760537 ]]\n"
     ]
    }
   ],
   "source": [
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_norm = norm.transform(X_train)\n",
    "print(\"Scaled Train Data: \\n\\n\")\n",
    "print(X_train_norm)\n",
    "\n",
    "# transform testing dataabs\n",
    "X_test_norm = norm.transform(X_test)\n",
    "print(\"\\n\\nScaled Test Data: \\n\\n\")\n",
    "print(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data standardization with  sklearn\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Train Data: \n",
      "\n",
      "\n",
      "[[-0.89377575 -0.35899568]\n",
      " [-0.21016664 -1.42687302]\n",
      " [ 0.6242858  -1.68966027]\n",
      " ...\n",
      " [-1.07136434  0.26616775]\n",
      " [ 1.8010777   1.45398019]\n",
      " [-1.37946986 -0.51773096]]\n",
      "\n",
      "\n",
      "Scaled Test Data: \n",
      "\n",
      "\n",
      "[[ 1.68339851 -1.67525307]\n",
      " [-1.07136434  1.77477721]\n",
      " [ 0.53870094  0.78357176]\n",
      " ...\n",
      " [-1.23718502 -0.5556443 ]\n",
      " [ 1.58711553  0.64665858]\n",
      " [-0.21016664 -0.73202999]]\n"
     ]
    }
   ],
   "source": [
    "# fit scaler on training data\n",
    "norm = StandardScaler().fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_norm = norm.transform(X_train)\n",
    "print(\"Scaled Train Data: \\n\\n\")\n",
    "print(X_train_norm)\n",
    "\n",
    "# transform testing dataabs\n",
    "X_test_norm = norm.transform(X_test)\n",
    "print(\"\\n\\nScaled Test Data: \\n\\n\")\n",
    "print(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>1115.721058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized</th>\n",
       "      <td>1150.923047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized</th>\n",
       "      <td>1150.923047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     RMSE\n",
       "Original      1115.721058\n",
       "Normalized    1150.923047\n",
       "Standardized  1150.923047"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training an SVR model\n",
    "from  sklearn.svm import SVR\n",
    "# measuring RMSE score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# SVR\n",
    "svr = SVR(kernel='rbf',C=5)\n",
    "\n",
    "rmse = []\n",
    "\n",
    "# raw, normalized and standardized training and testing data\n",
    "trainX = [X_train, X_train_norm, X_train_stand]\n",
    "testX = [X_test, X_test_norm, X_test_stand]\n",
    "\n",
    "# model fitting and measuring RMSE\n",
    "for i in range(len(trainX)):\n",
    "    \n",
    "    # fit\n",
    "    svr.fit(trainX[i],y_train)\n",
    "    # predict\n",
    "    pred = svr.predict(testX[i])\n",
    "    # RMSE\n",
    "    rmse.append(np.sqrt(mean_squared_error(y_test,pred)))\n",
    "\n",
    "# visualizing the result    \n",
    "df_svr = pd.DataFrame({'RMSE':rmse},index=['Original','Normalized','Standardized'])\n",
    "df_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>1169.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized</th>\n",
       "      <td>1334.253086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized</th>\n",
       "      <td>1148.844577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     RMSE\n",
       "Original      1169.491000\n",
       "Normalized    1334.253086\n",
       "Standardized  1148.844577"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training a KNN model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# measuring RMSE score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# knn \n",
    "knn = KNeighborsRegressor(n_neighbors=7)\n",
    "\n",
    "rmse = []\n",
    "\n",
    "# raw, normalized and standardized training and testing data\n",
    "trainX = [X_train, X_train_norm, X_train_stand]\n",
    "testX = [X_test, X_test_norm, X_test_stand]\n",
    "\n",
    "# model fitting and measuring RMSE\n",
    "for i in range(len(trainX)):\n",
    "    \n",
    "    # fit\n",
    "    knn.fit(trainX[i],y_train)\n",
    "    # predict\n",
    "    pred = knn.predict(testX[i])\n",
    "    # RMSE\n",
    "    rmse.append(np.sqrt(mean_squared_error(y_test,pred)))\n",
    "\n",
    "# visualizing the result\n",
    "df_knn = pd.DataFrame({'RMSE':rmse},index=['Original','Normalized','Standardized'])\n",
    "df_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Train Data: \n",
      "\n",
      "\n",
      "[[-0.4837523  -0.25694186]\n",
      " [-0.09196812 -0.98055353]\n",
      " [ 0.38626609 -1.15862259]\n",
      " ...\n",
      " [-0.58553035  0.16667941]\n",
      " [ 1.06069896  0.97156113]\n",
      " [-0.76210914 -0.36450355]]\n",
      "\n",
      "\n",
      "Scaled Test Data: \n",
      "\n",
      "\n",
      "[[ 0.99325567 -1.14886003]\n",
      " [-0.58553035  1.18893859]\n",
      " [ 0.33721643  0.51728108]\n",
      " ...\n",
      " [-0.68056407 -0.39019427]\n",
      " [ 0.9380748   0.42450641]\n",
      " [-0.09196812 -0.50971619]]\n"
     ]
    }
   ],
   "source": [
    "# fit scaler on training data\n",
    "norm = RobustScaler().fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_norm = norm.transform(X_train)\n",
    "print(\"Scaled Train Data: \\n\\n\")\n",
    "print(X_train_norm)\n",
    "\n",
    "# transform testing dataabs\n",
    "X_test_norm = norm.transform(X_test)\n",
    "print(\"\\n\\nScaled Test Data: \\n\\n\")\n",
    "print(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Absolute Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Train Data: \n",
      "\n",
      "\n",
      "[[0.40538642 0.44878533]\n",
      " [0.55503513 0.19883442]\n",
      " [0.73770492 0.13732556]\n",
      " ...\n",
      " [0.36651054 0.59511316]\n",
      " [0.99531616 0.87313649]\n",
      " [0.29906323 0.41163123]]\n",
      "\n",
      "\n",
      "Scaled Test Data: \n",
      "\n",
      "\n",
      "[[0.96955504 0.14069776]\n",
      " [0.36651054 0.9482233 ]\n",
      " [0.71896956 0.71621846]\n",
      " ...\n",
      " [0.33021077 0.40275711]\n",
      " [0.94847775 0.68417211]\n",
      " [0.55503513 0.36147169]]\n"
     ]
    }
   ],
   "source": [
    "# fit scaler on training data\n",
    "norm = MaxAbsScaler().fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_norm = norm.transform(X_train)\n",
    "print(\"Scaled Train Data: \\n\\n\")\n",
    "print(X_train_norm)\n",
    "\n",
    "# transform testing dataabs\n",
    "X_test_norm = norm.transform(X_test)\n",
    "print(\"\\n\\nScaled Test Data: \\n\\n\")\n",
    "print(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling to vector unit norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Train Data: \n",
      "\n",
      "\n",
      "[[0.07207221 0.99739942]\n",
      " [0.21793671 0.9759629 ]\n",
      " [0.3948213  0.91875793]\n",
      " ...\n",
      " [0.0492072  0.99878859]\n",
      " [0.09081317 0.99586795]\n",
      " [0.05802172 0.99831532]]\n",
      "\n",
      "\n",
      "Scaled Test Data: \n",
      "\n",
      "\n",
      "[[0.48276313 0.87575097]\n",
      " [0.03090555 0.99952231]\n",
      " [0.08004558 0.9967912 ]\n",
      " ...\n",
      " [0.06544615 0.9978561 ]\n",
      " [0.11022386 0.99390679]\n",
      " [0.12191652 0.99254036]]\n"
     ]
    }
   ],
   "source": [
    "# fit scaler on training data\n",
    "norm = Normalizer().fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_norm = norm.transform(X_train)\n",
    "print(\"Scaled Train Data: \\n\\n\")\n",
    "print(X_train_norm)\n",
    "\n",
    "# transform testing dataabs\n",
    "X_test_norm = norm.transform(X_test)\n",
    "print(\"\\n\\nScaled Test Data: \\n\\n\")\n",
    "print(X_test_norm)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Scaling your test data according to the train data makes sure that the test data is on the same scale as the training data on which our model was trained on. This way our model will be able to apply the learnings from the training dataset on the testing dataset, which is exactly what we want! If instead, we scale the test data differently, then our model might not be able to discern that difference, thereby giving us incorrect outputs. That way we will never know how well our model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
